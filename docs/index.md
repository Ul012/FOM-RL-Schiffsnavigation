# ğŸš¢ Q-Learning fÃ¼r Schiffsnavigation

Dieses Projekt implementiert einen Q-Learning-Algorithmus zur autonomen Navigation eines Agenten durch verschiedene Gitterumgebungen. Das System ermÃ¶glicht das Training und die Evaluation von Navigationsstrategien unter verschiedenen Umgebungsbedingungen mit einer modularen Code-Architektur.

## ğŸ¯ Projektziele

- Entwicklung eines robusten Q-Learning-Agenten fÃ¼r Navigationsprobleme
- Implementierung verschiedener Umgebungsszenarien mit unterschiedlichen KomplexitÃ¤tsgraden
- Bereitstellung von Evaluations- und Vergleichstools fÃ¼r wissenschaftliche Analyse
- Modulare Code-Architektur nach Clean Code Prinzipien
- Dokumentation der Implementierung und Ergebnisse

## ğŸ—ï¸ Systemarchitektur

Das Projekt besteht aus mehreren Komponenten:

- **Training**: Automatisiertes Lernen fÃ¼r einzelne oder multiple Szenarien
- **Evaluation**: Quantitative Analyse der gelernten Policies
- **Vergleich**: Statistische Auswertung verschiedener Szenarien
- **Visualisierung**: Grafische Darstellung der Agentenverhalten und Lernfortschritte
- **Utils-Module**: Wiederverwendbare Komponenten fÃ¼r Q-Learning Projekte

## ğŸ“ Projektstruktur

```
ship-navigation-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                    # Einzelszenario-Training
â”‚   â”œâ”€â”€ train_all_scenarios.py      # Multi-Szenario-Training
â”‚   â”œâ”€â”€ compare_scenarios.py        # Szenarien-Vergleich
â”‚   â”œâ”€â”€ evaluate_policy.py          # Policy-Evaluation
â”‚   â”œâ”€â”€ visualize_policy.py         # Visuelle Darstellung
â”‚   â”œâ”€â”€ inspect_q_tables.py         # Q-Tabellen-Analyse
â”‚   â”œâ”€â”€ config.py                   # Zentrale Konfiguration
â”‚   â”œâ”€â”€ envs/                       # Umgebungs-Implementierungen
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ grid_environment.py     # Grid-Umgebung
â”‚   â”‚   â””â”€â”€ container_environment.py # Container-Umgebung
â”‚   â””â”€â”€ utils/                      # Wiederverwendbare Module
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ common.py              # Basis-Hilfsfunktionen
â”‚       â”œâ”€â”€ environment.py         # Umgebungs-Initialisierung
â”‚       â”œâ”€â”€ qlearning.py           # Q-Learning Algorithmus
â”‚       â”œâ”€â”€ evaluation.py          # Bewertungslogik
â”‚       â”œâ”€â”€ position.py            # Position/State Konvertierungen
â”‚       â”œâ”€â”€ visualization.py       # Plotting-Funktionen
â”‚       â””â”€â”€ reporting.py           # Ausgabe-Funktionen
â”œâ”€â”€ exports/                        # Generierte Visualisierungen
â”œâ”€â”€ docs/                          # MkDocs Dokumentation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ—ºï¸ VerfÃ¼gbare Umgebungsszenarien

| Szenario | Beschreibung | Anwendungsbereich | Emojis |
|----------|--------------|-------------------|---------|
| **Static** | Konstante Positionen fÃ¼r alle Elemente | Grundlegendes Q-Learning | ğŸ§­ğŸğŸª¨ |
| **Random Start** | Variable Startpositionen | Robustheitstesting | ğŸš¢ğŸğŸª¨ |
| **Random Goal** | Variable Zielpositionen | Adaptive Navigation | ğŸ§­ğŸğŸª¨ |
| **Random Obstacles** | Variable Hindernisverteilungen | Dynamische Umgebungen | ğŸ§­ğŸğŸª¨ |
| **Container** | Pickup/Dropoff-Aufgaben | Komplexe Aufgabenstellungen | ğŸš¢ğŸ“¦ğŸ |

## âš™ï¸ Technische Spezifikationen

- **Umgebung**: 5x5 Gitterwelt (OpenAI Gymnasium-kompatibel)
- **Algorithmus**: Q-Learning mit Epsilon-Greedy-Exploration
- **Zustandsraum**: Diskret (25 ZustÃ¤nde fÃ¼r Grid, erweitert fÃ¼r Container)
- **Aktionsraum**: 4 Bewegungsrichtionen (Oben, Rechts, Unten, Links)
- **Terminierungsbedingungen**: Zielerreichung, Timeout, Schleifenerkennung, Hinderniskollision
- **Visualisierung**: Matplotlib mit PDF-Export, Pygame fÃ¼r interaktive Darstellung
- **Code-Struktur**: Modulare Python-Pakete mit Utils-Bibliothek

## ğŸ® Visualisierung

Das System verwendet intuitive Emojis fÃ¼r die visuelle Darstellung:

### Grid-Umgebungen
- ğŸš¢ **Agent/Schiff** - Aktuelle Position
- ğŸ§­ **Start** - Startposition (bei festen Starts)
- ğŸ **Ziel** - Zielposition
- ğŸª¨ **Hindernis** - Nicht passierbare Felder
- â†‘â†’â†“â† **Policy-Pfeile** - Optimale Aktionen der gelernten Policy

### Container-Umgebung
- ğŸš¢ **Schiff** - Agent-Position
- ğŸ“¦ **Pickup** - Container-Abholposition
- ğŸ **Dropoff** - Container-Abgabeposition (Ziel)
- ğŸª¨ **Hindernis** - Nicht passierbare Felder

## ğŸ“Š Wissenschaftliche Evaluierung

Das System bietet umfassende AnalysemÃ¶glichkeiten:

- **Erfolgsraten-Vergleich** zwischen verschiedenen Szenarien
- **Statistische Auswertung** von Terminierungsarten
- **Lernkurven-Analyse** mit Moving-Average-GlÃ¤ttung
- **Parameter-SensitivitÃ¤tsanalyse** durch zentrale Konfiguration
- **Reproduzierbare Experimente** mit Seed-Management
- **Diagramme** mit automatischem PDF-Export

## ğŸ—ï¸ Code-Architektur

Das Projekt folgt modernen Software-Engineering-Prinzipien:

- **DRY-Prinzip**: Keine Code-Duplikation durch Utils-Module
- **Modulare Struktur**: Klare Trennung von Verantwortlichkeiten
- **Clean Code**: Lesbare und wartbare Implementierung
- **Wiederverwendbarkeit**: Utils kÃ¶nnen in anderen RL-Projekten genutzt werden
- **Skalierbarkeit**: Einfache Erweiterung um neue Szenarien und Algorithmen

## âš™ï¸ Installation und Setup

### Virtuelle Umgebung erstellen
```bash
python -m venv rl-venv
rl-venv\Scripts\activate          # Windows
source rl-venv/bin/activate       # macOS/Linux
```

### AbhÃ¤ngigkeiten installieren
```bash
pip install -r requirements.txt
```

### Erste Schritte
```bash
cd src
python train.py                   # Training starten
python visualize_policy.py        # Ergebnisse visualisieren
```

---

**ğŸ“š WeiterfÃ¼hrende Informationen:**

- [Funktionsweise des Systems](funktionsweise.md)
- [Training und Konfiguration](training.md)
- [Visualisierung und Export](visualisierung.md)