# Willkommen

Mit diesem Projekt navigiert ein Reinforcement-Learning-Agent ein Schiff durch eine Gitterwelt.

## Ziele
- Entwicklung eines Q-Learning-Agents
- Erstellung einer eigenen OpenAI-Gymnasium-Umgebung
- Dokumentation des Projektverlaufs mit MkDocs

ğŸ‘‰ Siehe [Setup & AusfÃ¼hrung](setup.md)

---

## ğŸ”€ Modus-Ãœbersicht

| Modus         | Beschreibung                                                                 |
|---------------|------------------------------------------------------------------------------|
| `static`      | Fester Start-, Ziel- und Hindernisbereich. Gut zum Einstieg, stabile Lernkurve. |
| `random_start`| ZufÃ¤lliger Startpunkt bei festem Ziel. Testet Robustheit beim Startverhalten. |
| `random_goal` | Fester Startpunkt, aber zufÃ¤lliges Ziel. Erfordert flexible Zielnavigation.   |
| `random_obstacles` | Hindernisse variieren pro Episode. ErhÃ¶ht Unsicherheit, erschwert Lernen. |
| `container`   | Zwei-Ziel-Aufgabe: Container muss zuerst abgeholt, dann zum Ziel gebracht werden. |

---

## ğŸ“ Projektstruktur

```text
FOM-rl-shipnav-qlearning/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                     â† Q-Learning-Training mit Visualisierung & Erfolgsmetrik
â”‚   â”œâ”€â”€ evaluate_policy.py           â† Statistische Zielerreichung auf zufÃ¤lligen Karten
â”‚   â”œâ”€â”€ visualize_policy.py          â† Statische Darstellung der gelernten Policy mit Emojis & Exportfunktion
â”‚   â”œâ”€â”€ config.py                    â† Zentrale Steuerung des Szenarios Ã¼ber ENV_MODE
â”‚   â”œâ”€â”€ q_table.py                   â† Laden/Speichern von Q-Tabellen
â”‚   â””â”€â”€ navigation/
â”‚       â””â”€â”€ environment/
â”‚           â”œâ”€â”€ grid_environment.py      â† Basisumgebung
â”‚           â””â”€â”€ container_environment.py â† Container-Szenario mit Pickup/Dropoff
â”œâ”€â”€ requirements.txt                 â† ProjektabhÃ¤ngigkeiten
â”œâ”€â”€ mkdocs.yml                       â† MkDocs-Konfiguration
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ index.md                     â† Startseite
â”‚   â”œâ”€â”€ setup.md                     â† Setup-Anleitung fÃ¼r Umgebung, Training und Visualisierung
â””â”€â”€ site/                            â† (von mkdocs build erzeugt)
```

---

## Ablauf des Projekts

Der Ablauf des Projekts gliedert sich in drei Hauptschritte:

1. **Training**  
   Das Skript `train.py` erstellt eine Q-Tabelle durch Interaktion des Agenten mit der Umgebung. Dabei wird der Lernfortschritt Ã¼ber Rewards und Zielerreichung pro Episode dokumentiert. Ãœber die Variable `ENV_MODE` in `config.py` kann zwischen verschiedenen Szenarien wie `static`, `random_start`, `random_goal` oder `container` gewÃ¤hlt werden.

2. **Evaluation**  
   Mit `evaluate_policy.py` wird die gelernte Policy getestet â€“ z.â€¯B. in 100 zufÃ¤llig generierten Umgebungen. Es erfolgt kein Lernen mehr: Der Agent folgt der gespeicherten Q-Tabelle (`q_table.npy`) und wÃ¤hlt jeweils die beste bekannte Aktion. Ziel ist es, Erfolgsquote und durchschnittlichen Reward zu ermitteln. Es wird auch geprÃ¼ft, ob sich der Agent in einer Endlosschleife befindet.

3. **Visualisierung**  
   `visualize_policy.py` zeigt einen einzelnen Lauf des Agenten in der Umgebung animiert mit Pygame. Diese Darstellung dient der qualitativen Demonstration des Lernverhaltens. Optional kÃ¶nnen ein GIF und ein PDF-Screenshot exportiert werden.