{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Willkommen","text":"<p>Mit diesem Projekt navigiert ein Reinforcement-Learning-Agent ein Schiff durch eine Gitterwelt.</p>"},{"location":"#ziele","title":"Ziele","text":"<ul> <li>Entwicklung eines Q-Learning-Agents</li> <li>Erstellung einer eigenen OpenAI-Gymnasium-Umgebung</li> <li>Dokumentation des Projektverlaufs mit MkDocs</li> </ul> <p>\ud83d\udc49 Siehe Setup &amp; Ausf\u00fchrung</p>"},{"location":"#modus-ubersicht","title":"\ud83d\udd00 Modus-\u00dcbersicht","text":"Modus Beschreibung <code>static</code> Fester Start-, Ziel- und Hindernisbereich. Gut zum Einstieg, stabile Lernkurve. <code>random_start</code> Zuf\u00e4lliger Startpunkt bei festem Ziel. Testet Robustheit beim Startverhalten. <code>random_goal</code> Fester Startpunkt, aber zuf\u00e4lliges Ziel. Erfordert flexible Zielnavigation. <code>random_obstacles</code> Hindernisse variieren pro Episode. Erh\u00f6ht Unsicherheit, erschwert Lernen. <code>container</code> Zwei-Ziel-Aufgabe: Container muss zuerst abgeholt, dann zum Ziel gebracht werden."},{"location":"#projektstruktur","title":"\ud83d\udcc1 Projektstruktur","text":"<pre><code>FOM-rl-shipnav-qlearning/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 train.py                     \u2190 Q-Learning-Training mit Visualisierung &amp; Erfolgsmetrik\n\u2502   \u251c\u2500\u2500 evaluate_policy.py           \u2190 Statistische Zielerreichung auf zuf\u00e4lligen Karten\n\u2502   \u251c\u2500\u2500 visualize_policy.py          \u2190 Statische Darstellung der gelernten Policy mit Emojis &amp; Exportfunktion\n\u2502   \u251c\u2500\u2500 config.py                    \u2190 Zentrale Steuerung des Szenarios \u00fcber ENV_MODE\n\u2502   \u251c\u2500\u2500 q_table.py                   \u2190 Laden/Speichern von Q-Tabellen\n\u2502   \u2514\u2500\u2500 navigation/\n\u2502       \u2514\u2500\u2500 environment/\n\u2502           \u251c\u2500\u2500 grid_environment.py      \u2190 Basisumgebung\n\u2502           \u2514\u2500\u2500 container_environment.py \u2190 Container-Szenario mit Pickup/Dropoff\n\u251c\u2500\u2500 requirements.txt                 \u2190 Projektabh\u00e4ngigkeiten\n\u251c\u2500\u2500 mkdocs.yml                       \u2190 MkDocs-Konfiguration\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md                     \u2190 Startseite\n\u2502   \u251c\u2500\u2500 setup.md                     \u2190 Setup-Anleitung f\u00fcr Umgebung, Training und Visualisierung\n\u2514\u2500\u2500 site/                            \u2190 (von mkdocs build erzeugt)\n</code></pre>"},{"location":"#ablauf-des-projekts","title":"Ablauf des Projekts","text":"<p>Der Ablauf des Projekts gliedert sich in drei Hauptschritte:</p> <ol> <li> <p>Training    Das Skript <code>train.py</code> erstellt eine Q-Tabelle durch Interaktion des Agenten mit der Umgebung. Dabei wird der Lernfortschritt \u00fcber Rewards und Zielerreichung pro Episode dokumentiert. \u00dcber die Variable <code>ENV_MODE</code> in <code>config.py</code> kann zwischen verschiedenen Szenarien wie <code>static</code>, <code>random_start</code>, <code>random_goal</code> oder <code>container</code> gew\u00e4hlt werden.</p> </li> <li> <p>Evaluation    Mit <code>evaluate_policy.py</code> wird die gelernte Policy getestet \u2013 z.\u202fB. in 100 zuf\u00e4llig generierten Umgebungen. Es erfolgt kein Lernen mehr: Der Agent folgt der gespeicherten Q-Tabelle (<code>q_table.npy</code>) und w\u00e4hlt jeweils die beste bekannte Aktion. Ziel ist es, Erfolgsquote und durchschnittlichen Reward zu ermitteln. Es wird auch gepr\u00fcft, ob sich der Agent in einer Endlosschleife befindet.</p> </li> <li> <p>Visualisierung <code>visualize_policy.py</code> zeigt einen einzelnen Lauf des Agenten in der Umgebung animiert mit Pygame. Diese Darstellung dient der qualitativen Demonstration des Lernverhaltens. Optional k\u00f6nnen ein GIF und ein PDF-Screenshot exportiert werden.</p> </li> </ol>"},{"location":"dokumentation/","title":"Dokumentation und Weiterentwicklung","text":"<p>Diese Seite beschreibt, wie das Projekt dokumentiert und strukturiert ist.</p>"},{"location":"dokumentation/#mkdocs","title":"MkDocs","text":"<p>Die gesamte Dokumentation basiert auf MkDocs. \u00c4nderungen an der Seitenstruktur erfolgen \u00fcber <code>mkdocs.yml</code>.</p>"},{"location":"dokumentation/#github-integration-optional","title":"GitHub-Integration (optional)","text":"<p>Eine Ver\u00f6ffentlichung der Dokumentation \u00fcber GitHub Pages ist mit folgendem Befehl m\u00f6glich:</p> <pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"funktionsweise/","title":"Funktionsweise des Agenten","text":"<p>Dieses Kapitel beschreibt die grundlegende Funktionsweise des Q-Learning-Agenten.</p>"},{"location":"funktionsweise/#entscheidungslogik","title":"Entscheidungslogik","text":"<p>Der Agent w\u00e4hlt in jedem Zustand eine Aktion, die basierend auf der Q-Tabelle die h\u00f6chste Belohnung verspricht. Die Q-Tabelle wird dabei iterativ verbessert.</p>"},{"location":"funktionsweise/#belohnungsstruktur","title":"Belohnungsstruktur","text":"<ul> <li>Ziel erreicht: +10 Punkte</li> <li>Hindernis getroffen oder Schleife erkannt: -10 Punkte</li> <li>Jeder Schritt: -0.1 Punkte</li> </ul>"},{"location":"funktionsweise/#modussteuerung","title":"Modussteuerung","text":"<p>\u00dcber <code>config.py</code> kann der Modus gesteuert werden: - <code>static</code>, <code>random_start</code>, <code>random_goal</code>, <code>random_obstacles</code>, <code>container</code></p>"},{"location":"setup/","title":"Setup &amp; Ausf\u00fchrung","text":""},{"location":"setup/#lokale-ausfuhrung","title":"Lokale Ausf\u00fchrung","text":"<pre><code># Virtuelle Umgebung aktivieren\nrl-venv\\Scripts\\activate\n\n# Installieren der Abh\u00e4ngigkeiten\npip install -r requirements.txt\n\n# Trainingsskript starten\ncd src\npython train.py\n</code></pre>"},{"location":"setup/#evaluation","title":"Evaluation","text":"<pre><code>python evaluate_policy.py\n</code></pre> <ul> <li>Gibt Zielerreichungen, Rewards und Schleifenabbr\u00fcche aus</li> <li>Funktioniert mit verschiedenen ENV_MODE-Einstellungen</li> </ul>"},{"location":"setup/#visualisierung","title":"Visualisierung","text":"<pre><code>python visualize_policy.py\n</code></pre> <ul> <li>Zeigt Agentenlauf in Echtzeit</li> <li>Emojis f\u00fcr Start (\ud83e\udded), Ziel (\ud83c\udfc1), Hindernis (\ud83e\udea8), Pickup (\ud83d\udce4), Dropoff (\ud83d\udce6)</li> <li>Optionaler GIF- und PDF-Export durch <code>EXPORT_FRAMES = True</code></li> </ul>"},{"location":"setup/#dokumentation-lokal-aufrufen","title":"Dokumentation lokal aufrufen","text":"<pre><code>mkdocs serve\n</code></pre> <p>Dann im Browser \u00f6ffnen: http://127.0.0.1:8000</p>"},{"location":"setup/#abhangigkeiten-aktualisieren","title":"Abh\u00e4ngigkeiten aktualisieren","text":"<p>Wenn du bereits Pakete installiert hast und die <code>requirements.txt</code> auf den aktuellen Stand bringen m\u00f6chtest:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>Wenn du alle Pakete aus der <code>requirements.txt</code> auf die neuesten kompatiblen Versionen aktualisieren m\u00f6chtest:</p> <pre><code>pip install --upgrade -r requirements.txt\n</code></pre>"},{"location":"training/","title":"Training des Agenten","text":"<p>Das Training erfolgt \u00fcber <code>train.py</code>. In jeder Episode wird eine neue Umgebung generiert (abh\u00e4ngig vom Modus), und der Agent lernt durch Interaktion.</p>"},{"location":"training/#ablauf-einer-episode","title":"Ablauf einer Episode","text":"<ol> <li>Umgebung wird initialisiert</li> <li>Agent bewegt sich bis zum Ziel oder Timeout</li> <li>Q-Werte werden basierend auf Reward aktualisiert</li> </ol>"},{"location":"training/#hyperparameter","title":"Hyperparameter","text":"<ul> <li>Lernrate (<code>alpha</code>)</li> <li>Diskontfaktor (<code>gamma</code>)</li> <li>Explorationsrate (<code>epsilon</code>) mit optionalem Decay</li> </ul>"},{"location":"visualisierung/","title":"Visualisierung der Policy","text":"<p>Die Datei <code>visualize_policy.py</code> zeigt die Policy des Agenten als animierten Lauf in der Umgebung.</p>"},{"location":"visualisierung/#darstellungsmerkmale","title":"Darstellungsmerkmale","text":"<ul> <li>\ud83e\udded Startposition</li> <li>\ud83d\udea2 Aktuelle Agentenposition (wird bei jedem Schritt aktualisiert)</li> <li>\ud83c\udfc1 Zielposition</li> <li>\ud83e\udea8 Hindernisse</li> <li>\ud83d\udce4 Pickup-Punkt (Container abholen)</li> <li>\ud83d\udce6 Dropoff-Punkt (Container abliefern)</li> </ul> <p>Diese Emojis helfen bei der intuitiven Interpretation des Agentenverhaltens.</p>"},{"location":"visualisierung/#export","title":"Export","text":"<p>Wenn <code>EXPORT_FRAMES = True</code> gesetzt ist, erzeugt das Skript:</p> <ul> <li>eine animierte GIF-Datei des Agentenlaufs</li> <li>einen Screenshot der Zielerreichung als PDF</li> </ul>"}]}