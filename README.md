# ğŸš¢ Simulation von Schiffsnavigation mit Reinforcement Learning

Dieses Projekt zeigt, wie ein Q-Learning-Agent ein Schiff durch eine 5x5-Gitterwelt navigiert. Die Umgebung basiert auf OpenAI Gymnasium und wird visuell mit Pygame dargestellt. Die Dokumentation erfolgt mit MkDocs.

---

## ğŸ“Œ Inhalte

- Eigene Grid-Umgebung (OpenAI Gym-kompatibel)
- Q-Learning-Implementierung mit Trainingsschleife
- Verschiedene Umgebungsmodi:
  - Fester Start, Ziel und Hindernisse (`static`)
  - ZufÃ¤llige Start-/Zielpositionen (`random_goal`)
  - ZufÃ¤llige Hindernisse (`random_obstacles`)
  - Pickup/Dropoff-Szenario mit Containertransport (`pickup_dropoff`)
- Schleifen- und Timeout-Erkennung zur Stabilisierung
- Visualisierung der Policy und AgentenlÃ¤ufe mit Pygame
- Optionaler GIF- und PDF-Export zur Ergebnisdokumentation
- **Evaluation**: Zielerreichung, Reward, SchleifenabbrÃ¼che mit `evaluate_policy.py`
- Dokumentation mit MkDocs

---

## âš™ï¸ Setup

### 1. Virtuelle Umgebung erstellen und aktivieren
```bash
python -m venv venv
venv\Scripts\activate          # Windows
source venv/bin/activate       # macOS/Linux
```

### 2. AbhÃ¤ngigkeiten installieren
```bash
pip install -r requirements.txt
```

Falls Probleme mit `distutils` auftreten (Python 3.12):
```bash
pip install setuptools wheel
```

---

## ğŸš€ AusfÃ¼hrung

### 3. Training starten
```bash
cd src
python train.py
```

### 4. Policy visualisieren
```bash
python visualize_policy.py
```
- Darstellung mit ğŸ§­ Start, ğŸ Ziel, ğŸª¨ Hindernissen
- Export von GIF/PDF durch `EXPORT_FRAMES = True`

### 5. Evaluation durchfÃ¼hren
```bash
python evaluate_policy.py
```
- Zielerreichungen, durchschnittliche Rewards, SchleifenabbrÃ¼che
- Robuste Erfolgserkennung mit `reward >= 10`
- Automatisches Laden der passenden Q-Tabelle je nach Modus

---

## ğŸ“š Dokumentation (lokal)

```bash
mkdocs serve
```
Danach erreichbar unter [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## ğŸ”€ Modus-Ãœbersicht

| Modus         | Beschreibung                                                                 |
|---------------|------------------------------------------------------------------------------|
| `static`      | Fester Start-, Ziel- und Hindernisbereich. Gut zum Einstieg, stabile Lernkurve. |
| `random_start`| ZufÃ¤lliger Startpunkt bei festem Ziel. Testet Robustheit beim Startverhalten. |
| `random_goal` | Fester Startpunkt, aber zufÃ¤lliges Ziel. Erfordert flexible Zielnavigation.   |
| `random_obstacles` | Hindernisse variieren pro Episode. ErhÃ¶ht Unsicherheit, erschwert Lernen. |
| `container`   | Zwei-Ziel-Aufgabe: Container muss zuerst abgeholt, dann zum Ziel gebracht werden. |

---

## ğŸ“ Projektstruktur

```text
FOM-rl-shipnav-qlearning/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                    â† Q-Learning Training
â”‚   â”œâ”€â”€ evaluate_policy.py          â† Auswertung (Erfolg, Reward, Schleifen)
â”‚   â”œâ”€â”€ visualize_policy.py         â† Visualisierung mit Pygame
â”‚   â”œâ”€â”€ q_table.py                  â† Initialisierung und Speicherung von Q-Tabellen
â”‚   â”œâ”€â”€ config.py                   â† Konfiguration der Modi
â”‚   â””â”€â”€ navigation/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ environment/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ grid_environment.py     â† Basisumgebung
â”‚       â”‚   â””â”€â”€ container_environment.pyâ† Pickup/Dropoff-Modus
â”œâ”€â”€ q_table.npy                     â† Beispielhafte gespeicherte Q-Tabelle
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ mkdocs.yml
â”œâ”€â”€ .gitignore
â””â”€â”€ docs/
    â”œâ”€â”€ index.md                    â† Startseite der MkDocs-Dokumentation
    â””â”€â”€ setup.md                    â† Installationsanleitung
```