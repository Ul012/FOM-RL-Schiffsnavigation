# ğŸš¢ Q-Learning fÃ¼r Schiffsnavigation

Dieses Projekt implementiert Q-Learning zur autonomen Navigation eines Schiffes durch eine 5x5-Gitterwelt. Das System umfasst verschiedene Umgebungsmodi, automatisiertes Multi-Szenario-Training und umfassende Evaluationstools.

## ğŸ“‹ ProjektÃ¼bersicht

- **Q-Learning-Implementierung** mit automatisiertem Training
- **Gymnasium-kompatible Umgebungen** fÃ¼r verschiedene Navigationsszenarien
- **Multi-Szenario-Training** mit `train_all_scenarios.py`
- **Szenarien-Vergleich** mit statistischer Analyse
- **Robuste Terminierungserkennung** (Erfolg, Timeout, Schleifen, Hindernisse)
- **Professionelle Visualisierung** und PDF-Export
- **Modulare Code-Architektur** mit wiederverwendbaren Utils
- **Umfassende Dokumentation** mit MkDocs

## âš™ï¸ Installation

### Virtuelle Umgebung erstellen
```bash
python -m venv rl-venv
rl-venv\Scripts\activate          # Windows
source rl-venv/bin/activate       # macOS/Linux
```

### AbhÃ¤ngigkeiten installieren
```bash
pip install -r requirements.txt
```

## ğŸš€ Verwendung

### Einzelnes Szenario trainieren
```bash
cd src
python train.py
```

### Alle Szenarien automatisch trainieren
```bash
python train_all_scenarios.py
```

### Szenarien vergleichen
```bash
python compare_scenarios.py
```

### Policy evaluieren
```bash
python evaluate_policy.py
```

### Visuell darstellen
```bash
python visualize_policy.py
```

### Q-Tabellen inspizieren
```bash
python inspect_q_tables.py
```

## ğŸ—ºï¸ VerfÃ¼gbare Szenarien

| Szenario | Beschreibung | KomplexitÃ¤t | Emojis |
|----------|--------------|-------------|---------|
| `static` | Feste Positionen fÃ¼r Start, Ziel und Hindernisse | Niedrig | ğŸ§­ğŸğŸª¨ |
| `random_start` | ZufÃ¤llige Startposition bei festem Ziel | Mittel | ğŸš¢ğŸğŸª¨ |
| `random_goal` | Feste Startposition mit zufÃ¤lligem Ziel | Mittel | ğŸ§­ğŸğŸª¨ |
| `random_obstacles` | Variable Hindernispositionen pro Episode | Hoch | ğŸ§­ğŸğŸª¨ |
| `container` | Pickup/Dropoff-Aufgabe mit erweiterten ZustÃ¤nden | Sehr hoch | ğŸš¢ğŸ“¦ğŸ |

## âš™ï¸ Konfiguration

Alle Parameter werden zentral in `config.py` verwaltet:

```python
ENV_MODE    # Szenario-Auswahl
EPISODES    # Trainings-Episoden
MAX_STEPS   # Maximale Schritte pro Episode
ALPHA       # Lernrate
GAMMA       # Diskontierungsfaktor
EPSILON     # Explorationsrate
```

## ğŸ“ Projektstruktur

```
ship-navigation-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                    # Einzelszenario-Training
â”‚   â”œâ”€â”€ train_all_scenarios.py      # Multi-Szenario-Training
â”‚   â”œâ”€â”€ compare_scenarios.py        # Szenarien-Vergleich
â”‚   â”œâ”€â”€ evaluate_policy.py          # Policy-Evaluation
â”‚   â”œâ”€â”€ visualize_policy.py         # Visuelle Darstellung
â”‚   â”œâ”€â”€ inspect_q_tables.py         # Q-Tabellen-Analyse
â”‚   â”œâ”€â”€ config.py                   # Zentrale Konfiguration
â”‚   â”œâ”€â”€ envs/                       # Umgebungs-Implementierungen
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ grid_environment.py     # Grid-Umgebung
â”‚   â”‚   â””â”€â”€ container_environment.py # Container-Umgebung
â”‚   â””â”€â”€ utils/                      # Wiederverwendbare Module
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ common.py              # Basis-Hilfsfunktionen
â”‚       â”œâ”€â”€ environment.py         # Umgebungs-Initialisierung
â”‚       â”œâ”€â”€ qlearning.py           # Q-Learning Algorithmus
â”‚       â”œâ”€â”€ evaluation.py          # Bewertungslogik
â”‚       â”œâ”€â”€ position.py            # Position/State Konvertierungen
â”‚       â”œâ”€â”€ visualization.py       # Plotting-Funktionen
â”‚       â””â”€â”€ reporting.py           # Ausgabe-Funktionen
â”œâ”€â”€ exports/                        # Generierte Visualisierungen
â”œâ”€â”€ docs/                          # MkDocs Dokumentation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ® Visualisierung

Das System verwendet intuitive Emojis fÃ¼r die visuelle Darstellung:

### Grid-Umgebungen
- ğŸš¢ **Agent/Schiff** - Aktuelle Position
- ğŸ§­ **Start** - Startposition (bei festen Starts)
- ğŸ **Ziel** - Zielposition
- ğŸª¨ **Hindernis** - Nicht passierbare Felder
- â†‘â†’â†“â† **Policy-Pfeile** - Optimale Aktionen der gelernten Policy

### Container-Umgebung
- ğŸš¢ **Schiff** - Agent-Position
- ğŸ“¦ **Pickup** - Container-Abholposition
- ğŸ **Dropoff** - Container-Abgabeposition (Ziel)
- ğŸª¨ **Hindernis** - Nicht passierbare Felder

## ğŸ“Š Wissenschaftliche Evaluierung

AnalysemÃ¶glichkeiten:

- **Erfolgsraten-Vergleich** zwischen Szenarien
- **Terminierungsarten-Analyse** (Erfolg, Timeout, Schleifen, Hindernisse)
- **Lernkurven-Visualisierung** mit statistischen Metriken
- **Parameter-Logging** fÃ¼r Reproduzierbarkeit
- **Diagramme** mit PDF-Export

## ğŸ—ï¸ Code-Architektur

Das Projekt folgt modernen Software-Engineering-Prinzipien:

- **DRY-Prinzip**: Keine Code-Duplikation durch Utils-Module
- **Modulare Struktur**: Klare Trennung von Verantwortlichkeiten
- **Clean Code**: Lesbare und wartbare Implementierung
- **Wiederverwendbarkeit**: Utils kÃ¶nnen in anderen RL-Projekten genutzt werden

## ğŸ“š Dokumentation

Lokale Dokumentation starten:
```bash
mkdocs serve
```

VerfÃ¼gbar unter: http://127.0.0.1:8000

## ğŸ”§ Technische Details

- **Framework**: OpenAI Gymnasium
- **Algorithmus**: Q-Learning mit Epsilon-Greedy-Exploration
- **Visualisierung**: Matplotlib mit PDF-Export, Pygame fÃ¼r interaktive Darstellung
- **Dokumentation**: MkDocs mit Material Theme
- **Code-Struktur**: Modulare Python-Pakete mit Utils-Bibliothek