# ğŸš¢ Q-Learning fÃ¼r Schiffsnavigation

Dieses Projekt implementiert Q-Learning zur autonomen Navigation eines Schiffes durch eine 5x5-Gitterwelt. Das System umfasst verschiedene Umgebungsmodi, automatisiertes Multi-Szenario-Training und umfassende Evaluationstools.

## ğŸ“‹ ProjektÃ¼bersicht

- **Q-Learning-Implementierung** mit automatisiertem Training
- **Gymnasium-kompatible Umgebungen** fÃ¼r verschiedene Navigationsszenarien
- **Multi-Szenario-Training** mit `train_all_scenarios.py`
- **Szenarien-Vergleich** mit statistischer Analyse
- **Robuste Terminierungserkennung** (Erfolg, Timeout, Schleifen, Hindernisse)
- **Professionelle Visualisierung** und PDF-Export
- **Umfassende Dokumentation** mit MkDocs

## âš™ï¸ Installation

### Virtuelle Umgebung erstellen
```bash
python -m venv rl-venv
rl-venv\Scripts\activate          # Windows
source rl-venv/bin/activate       # macOS/Linux
```

### AbhÃ¤ngigkeiten installieren
```bash
pip install -r requirements.txt
```

## ğŸš€ Verwendung

### Einzelnes Szenario trainieren
```bash
cd src
python train.py
```

### Alle Szenarien automatisch trainieren
```bash
python train_all_scenarios.py
```

### Szenarien vergleichen
```bash
python compare_scenarios.py
```

### Policy evaluieren
```bash
python evaluate_policy.py
```

### Visuell darstellen
```bash
python visualize_policy.py
```

## ğŸ—ºï¸ VerfÃ¼gbare Szenarien

| Szenario | Beschreibung | KomplexitÃ¤t |
|----------|--------------|-------------|
| `static` | Feste Positionen fÃ¼r Start, Ziel und Hindernisse | Niedrig |
| `random_start` | ZufÃ¤llige Startposition bei festem Ziel | Mittel |
| `random_goal` | Feste Startposition mit zufÃ¤lligem Ziel | Mittel |
| `random_obstacles` | Variable Hindernispositionen pro Episode | Hoch |
| `container` | Pickup/Dropoff-Aufgabe mit erweiterten ZustÃ¤nden | Sehr hoch |

## âš™ï¸ Konfiguration

Alle Parameter werden zentral in `config.py` verwaltet:

```python
ENV_MODE    # Szenario-Auswahl
EPISODES    # Trainings-Episoden
MAX_STEPS   # Maximale Schritte pro Episode
ALPHA       # Lernrate
GAMMA       # Diskontierungsfaktor
EPSILON     # Explorationsrate
```

## ğŸ“ Projektstruktur

```
ship-navigation-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                    # Einzelszenario-Training
â”‚   â”œâ”€â”€ train_all_scenarios.py      # Multi-Szenario-Training
â”‚   â”œâ”€â”€ compare_scenarios.py        # Szenarien-Vergleich
â”‚   â”œâ”€â”€ evaluate_policy.py          # Policy-Evaluation
â”‚   â”œâ”€â”€ visualize_policy.py         # Visuelle Darstellung
â”‚   â”œâ”€â”€ inspect_q_tables.py         # Q-Tabellen-Analyse
â”‚   â”œâ”€â”€ config.py                   # Zentrale Konfiguration
â”‚   â””â”€â”€ navigation/
â”‚       â””â”€â”€ environment/
â”‚           â”œâ”€â”€ grid_environment.py      # Grid-Umgebung
â”‚           â””â”€â”€ container_environment.py # Container-Umgebung
â”œâ”€â”€ exports/                        # Generierte Visualisierungen
â”œâ”€â”€ docs/                          # MkDocs Dokumentation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Š Wissenschaftliche Evaluierung

AnalysemÃ¶glichkeiten:

- **Erfolgsraten-Vergleich** zwischen Szenarien
- **Terminierungsarten-Analyse** (Erfolg, Timeout, Schleifen, Hindernisse)
- **Lernkurven-Visualisierung** mit statistischen Metriken
- **Parameter-Logging** fÃ¼r Reproduzierbarkeit
- **Diagramme** mit PDF-Export

## ğŸ“š Dokumentation

Lokale Dokumentation starten:
```bash
mkdocs serve
```

VerfÃ¼gbar unter: http://127.0.0.1:8000

## ğŸ”§ Technische Details

- **Framework**: OpenAI Gymnasium
- **Algorithmus**: Q-Learning mit Epsilon-Greedy-Exploration
- **Visualisierung**: Matplotlib mit PDF-Export
- **Dokumentation**: MkDocs mit Material Theme